1)Characteristics of BIG DATA
The characteristics of BIG DATA are:
*Volume
*Velocity
*Variety
*Veracity

->Volume:
Big data implies enormous volumes of data. It used to be employees created data. Now that data is generated by machines, networks and human interaction on systems like social media the volume of data to be analyzed is massive. Yet, Inderpal states that the volume of data is not as much the problem as other V’s like veracity.

->Velocity:
Big Data Velocity deals with the pace at which data flows in from sources like business processes, machines, networks and human interaction with things like social media sites, mobile devices, etc. The flow of data is massive and continuous. This real-time data can help researchers and businesses make valuable decisions that provide strategic competitive advantages and ROI if you are able to handle the velocity. Inderpal suggest that sampling data can help deal with issues like volume and velocity.

->Variety:
Variety refers to the many sources and types of data both structured and unstructured. We used to store data from sources like spreadsheets and databases. Now data comes in the form of emails, photos, videos, monitoring devices, PDFs, audio, etc. This variety of unstructured data creates problems for storage, mining and analyzing data. Jeff Veis, VP Solutions at HP Autonomy presented how HP is helping organizations deal with big challenges including data variety.

->Veracity:
Big Data Veracity refers to the biases, noise and abnormality in data. Is the data that is being stored, and mined meaningful to the problem being analyzed. Inderpal feel veracity in data analysis is the biggest challenge when compares to things like volume and velocity. In scoping out your big data strategy you need to have your team and partners work to help keep your data clean and processes to keep ‘dirty data’ from accumulating in your systems.

There are other V's like
*Validity
*Volatility

->Validity:
Like big data veracity is the issue of validity meaning is the data correct and accurate for the intended use. Clearly valid data is key to making the right decisions. Phil Francisco, VP of Product Management from IBM spoke about IBM’s big data strategy and tools they offer to help with data veracity and validity.

->Volatility:
Big data volatility refers to how long is data valid and how long should it be stored. In this world of real time data you need to determine at what point is data no longer relevant to the current analysis.
Big data clearly deals with issues beyond volume, variety and velocity to other concerns like veracity, validity and volatility.





2)Possible solutions for solving bigdata

The possible solutions for solving big data are
*Scale-Up
*Scale-Down

->Scale-Up:
1)Increase the configuration of a single system.
2)Like disk capacity, RAM, data transfer speed.
3)Complex, costly, and time consuming process.

 ->Scale-Out:
 1)Use multiple commodity (economical) machines and distribute the load of
storage/processing among them.
2)Economical and quick to implement as it focuses on distribution of load.
3)Instead of having a single system with 10 TB of storage and 80 GB of RAM,
use 40 machines with 256 GB of storage and 2 GB of RAM.



3)scale-up vs scale_out
->Scale-Up:

Scaling up is taking what you’ve got, and replacing it with something more powerful. From a networking perspective, this could be taking a 1GbE switch, and replacing it with a 10GbE switch. Same number of switchports, but the bandwidth has been scaled up via bigger pipes. The 1GbE bottleneck has been relieved by the 10GbE replacement.

Scaling up is a viable scaling solution until it is impossible to scale up individual components any larger. For example, 10GbE is a practical limit for uplinking hosts to the network until such time as 25GbE and higher ports are readily available on hosts. In that context, what happens when 10GbE is no longer enough bandwidth for the uplinked host at that time rather than scaling up,we scale out.

->Scale-out:
Scaling out takes the infrastructure you’ve got, and replicates it to work in parallel. This has the effect of increasing infrastructure capacity roughly linearly. Data centers often scale out using pods. Build a compute pod, spin up applications to use it, then scale out by building another pod to add capacity. Actual application performance may not be linear, as application architectures must be written to work effectively in a scale-out environment.

Application delivery controllers (A10 Networks, F5 Networks) are examples of networking tools that help with scaling out. ADCs host a virtual IP that is the front end to pool members (real servers) on the back end. As the demand for an application grows, the application can be scaled out by adding additional pool members behind the virtual IP.

Leaf-spine network architectures are also “scale out” designs. As a new pod or rack is installed, top of rack leaf switches plumbed to the spine layer add capacity.
