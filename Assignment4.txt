1)Characteristics of BIG DATA

Volume refers to the vast amounts of data generated every second. Just think of all the emails, twitter messages, photos, video clips, sensor data etc. we produce and share every second. We are not talking Terabytes but Zettabytes or Brontobytes. On Facebook alone we send 10 billion messages per day, click the "like' button 4.5 billion times and upload 350 million new pictures each and every day. If we take all the data generated in the world between the beginning of time and 2008, the same amount of data will soon be generated every minute! This increasingly makes data sets too large to store and analyse using traditional database technology. With big data technology we can now store and use these data sets with the help of distributed systems, where parts of the data is stored in different locations and brought together by software.

Velocity refers to the speed at which new data is generated and the speed at which data moves around. Just think of social media messages going viral in seconds, the speed at which credit card transactions are checked for fraudulent activities, or the milliseconds it takes trading systems to analyse social media networks to pick up signals that trigger decisions to buy or sell shares. Big data technology allows us now to analyse the data while it is being generated, without ever putting it into databases.

Variety refers to the different types of data we can now use. In the past we focused on structured data that neatly fits into tables or relational databases, such as financial data (e.g. sales by product or region). In fact, 80% of the world’s data is now unstructured, and therefore can’t easily be put into tables (think of photos, video sequences or social media updates). With big data technology we can now harness differed types of data (structured and unstructured) including messages, social media conversations, photos, sensor data, video or voice recordings and bring them together with more traditional, structured data.

Veracity refers to the messiness or trustworthiness of the data. With many forms of big data, quality and accuracy are less controllable (just think of Twitter posts with hash tags, abbreviations, typos and colloquial speech as well as the reliability and accuracy of content) but big data and analytics technology now allows us to work with these type of data. The volumes often make up for the lack of quality or accuracy.

Value: Then there is another V to take into account when looking at Big Data: Value! It is all well and good having access to big data but unless we can turn it into value it is useless. So you can safely argue that 'value' is the most important V of Big Data. It is important that businesses make a business case for any attempt to collect and leverage big data. It is so easy to fall into the buzz trap and embark on big data initiatives without a clear understanding of costs and benefits.



2)Possible solutions for solving bigdata

the 2 main solutions are SCALE-UP AND SACLE-OUT

1. Scale Up
• Increase the configuration of a single system
• Like disk capacity, RAM, data transfer speed
• Complex, costly, and time consuming process
2. Scale Out
• Use multiple commodity (economical) machines and distribute the load of
storage/processing among them
• Economical and quick to implement as it focuses on distribution of load
• Instead of having a single system with 10 TB of storage and 80 GB of RAM,
use 40 machines with 256 GB of storage and 2 GB of RAM



3)scale-up vs scale_out

To increase the capacity and performance of the array more shelves full of disks were added to the same controllers.

Scale-Up Expansion
This kind of expansion is what is known as “scale-up” in storage terminology.  At first, adding more disks improves performance because disk throughput was probably the limiting factor from a performance standpoint. However, as the load on the array climbed — a situation often driven by virtualization — and more disks were added, the two controllers themselves became a bottleneck as each began to require more and more CPU for RAID calculations.  Eventually, enough disks were added that the controllers were simply saturated and could do no more. Adding more and faster disks behind an overloaded controller pair simply places more overload on the controllers. In these systems once the controllers are the bottleneck there is little you can do, apart from buying an additional new array with its own pair of controllers and moving some of the workload (VMs) onto the new array.

Scale-Out Expansion

While scaling out isn’t necessarily a new concept, it has gained new life as storage startups create new structures that leverage the power of modern x86 servers in order to address the limitations of older scale-up architectures. Rather than using a custom controller platform, these new scale-out storage systems use a group of x86 servers to form the storage cluster. Each server is loaded with disks and uses a network — usually Ethernet — to talk to the other servers in the storage array. The group of servers together forms a clustered storage array and provides LUNs or file shares over a network just like a traditional array. Adding additional nodes of disks to a scale out array actually adds another x86 server to the cluster.  At the same time, doing so adds more network ports, more CPU and more RAM. As the capacity of a scale-out array increases so does its performance, adding nodes usually results in linear performance improvements. Adding nodes is generally non-disruptive, so a normal maintenance window can be used to add capacity and performance to the array, rather than a full system outage.

Scale-Out as Needs Arise

A major benefit to these emerging scale out solutions is that the cost of upgrades is generally low; additional nodes are generally the same price as the first nodes, and there is very little operational impact to adding nodes. This means that it is sensible to start with enough capacity and performance for current demand plus a few months of growth, buying new nodes as consumption increases. There is no need to buy three years’ worth of forecast growth in the initial purchase.  This also means that it is quick and fairly easy to add capacity to cope with the large new user count for a new project, reorganization or company acquisition.  Agility is good.

Scale for Performance

The other major benefit is the ability to reach very high performance levels or extreme capacity. If each node has a modest 20TB usable capacity and delivers 20,000 IOPS then a twenty node cluster should deliver 400TB and 400,000 IOPS. Of course at some point even a scale-out array will stop scaling; the design of the cluster software will determine how many nodes the cluster can handle. Some scale out storage is limited to eight server nodes; other arrays will work with dozens or hundreds of nodes. If the array architecture scales to a hundred nodes then these hero numbers are Petabytes and a millions of IOPS.

Scale-Out Risks

Naturally there are risks in the scale-out architecture. The first is that each of those x86 servers is a failure domain that didn’t exist in the scale up environment. This is usually handled by the layout of data on the array; copies of all data are kept on at least two nodes (this is called replica-based data protection). Another risk is that code upgrades to the nodes are more complex; rolling out new software to a hundred nodes isn’t a trivial task. Luckily these architectures have been around for a while so update mechanisms tend to be built in and nodes with disparate code versions tends to be handled automatically. Do make sure the upgrade methodology for any array you buy suits your use. There is also a possible issue if the configuration of the nodes is fixed; the balance of capacity and performance in each node may not match your requirements. If you need a high performance array for a small amount of very hot data like a transactional database then your balance will be very different from someone who needs to house a large cold data set like a document management system. Make sure you won’t need to buy an excessive number of nodes to meet either your capacity or performance requirements.